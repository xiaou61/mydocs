---
# 当前页面内容标题
title: 爬虫实战2023
# 当前页面图标
icon: chrome
# 分类
category:
- python
- 爬虫
# 标签
tag:
- python
- 爬虫
sticky: false
# 是否收藏在博客主题的文章列表中，当填入数字时，数字越大，排名越靠前。
star: true
# 是否将该文章添加至文章列表中
article: true
# 是否将该文章添加至时间线中
timeline: true

---



## 1.实战豆瓣top250

这例用bs加上requests

网址是豆瓣的top250

豆瓣电影 Top 250 (douban.com)

还是一个大概的框架



```python
def main(page):
   url = 'https://movie.douban.com/top250?start='+ str(page*25)+'&filter='
   html = request_douban(url)
   soup = BeautifulSoup(html, 'lxml')
   save_to_excel(soup)
```



首先保存请求豆瓣的。



```python
def request_douban(url):
   try:
       response = requests.get(url)
       if response.status_code == 200:
           return response.text
   except requests.RequestException:
       return None
```

之后进行解析。

之后用bs4进行解析。

```python
list = soup.find(class_='grid_view').find_all('li')

   for item in list:
       item_name = item.find(class_='title').string
       item_img = item.find('a').find('img').get('src')
       item_index = item.find(class_='').string
       item_score = item.find(class_='rating_num').string
       item_author = item.find('p').text
       item_intr = item.find(class_='inq').string
```

之后可以将这些数据存储到excel表格里面

这里用到xlwt

```python
book=xlwt.Workbook(encoding='utf-8',style_compression=0)

sheet=book.add_sheet('豆瓣电影Top250',cell_overwrite_ok=True)
sheet.write(0,0,'名称')
sheet.write(0,1,'图片')
sheet.write(0,2,'排名')
sheet.write(0,3,'评分')
sheet.write(0,4,'作者')
sheet.write(0,5,'简介')
```

最后进行一下保存

```python
book.save(u'豆瓣最受欢迎的250部电影.xlsx')
```

就可以了。

## 2.**爬取1000首背景音乐**

这里选取的网站是

动态配乐模板下载_精美好看的配乐模板大全_图客巴巴 (tuke88.com)



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403809.png)



我们发现这个网站的爬取是非常的简单的。

![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403806.png)

结构非常的简单。



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403819.png)

这个分页的规律也很简单



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403816.png)

具体就是lmt-> audio-list 定位到a



```python
html.xpath("//div[@class='lmt']//div[@class='audio-list']//a[@class='title']/text()")
```

这个就是title的，之后的audio也是同理。



```python
import requests
import lxml.etree

page_n= int(input("请输入你想要爬取的网页数量"))

for i in range(page_n):
    url=f'https://www.tuke88.com/peiyue/zonghe_0_{i}.html'
    response = requests.get(url)

    # 修改html中的语法错误
    html_parser = lxml.etree.HTMLParser()
    html = lxml.etree.fromstring(response.text, parser=html_parser)
    titles = html.xpath("//div[@class='lmt']//div[@class='audio-list']//a[@class='title']/text()")
    mp3_urls=html.xpath("//div[@class='lmt']//div[@class='audio-list']//source/@src")
    print(mp3_urls)
```

这样就可以了，之后我们要把文件存储到本地。

之后进行存储



```python
if not os.path.exists('pymp3'):
    os.makedirs('pymp3')
for title,mp3_urls in zip(titles,mp3_urls):
    #mp3一般流下载
    mp3_stream = requests.get(mp3_urls, stream=True)
    with open(os.path.join('pymp3',title+".mp3"),'wb+') as writer:
        writer.write(mp3_stream.raw.read())
        print(f'【Info】{title}.mp3下载成功')
```



之后加入随机时间



```
time.sleep(random.uniform(0.1,0.4))
```



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403813.png)

这样就可以了。下面就是我们的全部源码



```python
import random
import time
import requests
import lxml.etree
import os
page_n= int(input("请输入你想要爬取的网页数量"))

for i in range(page_n):
    url=f'https://www.tuke88.com/peiyue/zonghe_0_{i}.html'
    response = requests.get(url)

    # 修改html中的语法错误
    html_parser = lxml.etree.HTMLParser()
    html = lxml.etree.fromstring(response.text, parser=html_parser)
    titles = html.xpath("//div[@class='lmt']//div[@class='audio-list']//a[@class='title']/text()")
    mp3_urls=html.xpath("//div[@class='lmt']//div[@class='audio-list']//source/@src")
    print(mp3_urls)
    if not os.path.exists('pymp3'):
        os.makedirs('pymp3')
    for title,mp3_urls in zip(titles,mp3_urls):
        #mp3一般流下载
        mp3_stream = requests.get(mp3_urls, stream=True)
        with open(os.path.join('pymp3',title+".mp3"),'wb+') as writer:
            writer.write(mp3_stream.raw.read())
            print(f'【Info】{title}.mp3下载成功')
            time.sleep(random.uniform(0.1,0.4))
```

之后我们来详细说一下这个流的下载，和我们普通的有什么不同



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403828.png)

如果为false，他就是立即下载的意思。



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403339.png)

## 3.爬取csdn

![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403398.png)

这里的JavaScript就不做过多的讲解了。

我们首先来进行一个分析

### 博客主页博客文章分析



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403445.png)



我们发现我们往下滑的时候，明显是有ajax请求的。

看我们的请求参数



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403500.png)

我们发现这个username就是一个博主姓名的标志

以及他的一些返回信息。

![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403565.png)



这里可以用articledID或者是url都可以。我们路由用

articledId

下面我们开始正式爬取

### 爬取页面数据

我们分析可以看到



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403623.png)



需要的数据在这里div id是content_views



```python
def crawler_blog_by(author_name,article_id,title,i):
     article_request_url=f"https://blog.csdn.net/{author_name}/article/details/{article_id}"
     response = sess.get(article_request_url)

     selector = etree.HTML(response.text)
     head_msg = selector.xpath(r"//head")[0]
     head_str = etree.tostring(head_msg, encoding='utf-8', method='html').decode()
     body_msg = selector.xpath(r"//div[@id='content_views]'")
     body_str = etree.tostring(body_msg, encoding="utf-8", method="html").decode()
    
```

我们有了之后，我们就保存为html就可以了



```python
if not os.path.exists("c_articles"):
    os.makedirs("c_articles")
# title进行一些优化
title = title.title("/", "-").title(":", "").title(": ", "")

save_file_name = os.path.join("c_articles", f'{author_name}-{title}-{article_id}.html')
with open(save_file_name, 'w', encoding='utf-8') as writer:
    writer.write(f"""<head> <meta charset="utf-8 </head>
        {body_str}
    """)
    print(f'【info】: {author_name}第{i}篇博文{title}-{article_id}.html保存文件成功')
    i += 1

```

之后我们开始循环爬取



```python
# 循环爬取分页html
for page_no in range(MAX_PAGE_NUM):
    try:
        data = {
            'page': '2',
            'size': '20',
            'businessType': 'blog',
            'orderby': '',
            'noMore': 'false',
            'year': '',
            'month': '',
            'username': author_name}
        pages_dict = sess.get('https://blog.csdn.net/community/home-api/v1/get-business-list', params=data).json()
        for article in pages_dict['data']['list']:
            article_id = article['articleId']
            title = article['title']
            crawler_blog_by(author_name,article_id,title,i)

        time.sleep(random.uniform(0.4,1.0))
    except Exception as e:
        print(e) #log日志文件系统，实际开发的话
```





![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403941.png)



这里就是对数据简单的进行了一些分析

这里有一些bug



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403994.png)



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403058.png)



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403120.png)

那么我们怎么办呢

我们给他定义成全局的i



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403160.png)



这样就可以了

之后我们来看如何下载这个pdf的插件

wkhtmltopdf

官网是在这里的

然后配置环境变量就可以了

### 保存为pdf文件



```python
def html_to_pdf(file_html_name):
    pre_file_name = os.path.splitext(file_html_name)[0]#这里的0就是.html前面，1就是html
    pdfkit.from_file(file_html_name,pre_file_name+".pdf")
```



很简单

下面我们就来看全部的源码



```python
"""
1 爬取博主的所有博文的article_ids
2 根据id爬取html
3 保存为html 在保存一个pdf格式
"""
import random
import time

import requests
from lxml import etree
import pdfkit
import os

author_name = input("输入博主ID:")
MAX_PAGE_NUM = 200
i = 1

sess = requests.Session()
agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.58"
sess.headers['User-Agent'] = agent

def html_to_pdf(file_html_name):

    pre_file_name = os.path.splitext(file_html_name)[0]#这里的0就是.html前面，1就是html
    pdfkit.from_file(file_html_name,pre_file_name+".pdf")

def crawler_blog_by(author_name, article_id, title):
    article_request_url = f"https://blog.csdn.net/{author_name}/article/details/{article_id}"

    response = sess.get(article_request_url)

    selector = etree.HTML(response.text)



    head_msg = selector.xpath(r"//head")[0]
    head_str = etree.tostring(head_msg, encoding='utf-8', method='html').decode()
    body_msg = selector.xpath(r'//div[@id="content_views"]')[0]
    body_str = etree.tostring(body_msg, encoding="utf-8", method="html").decode()


    if not os.path.exists("c_articles"):
        os.makedirs("c_articles")
    # title进行一些优化
    title = title.replace("/", "-").replace(":", "").replace(": ", "")

    save_file_name = os.path.join("c_articles", f'{author_name}-{title}-{article_id}.html')
    with open(save_file_name, 'w', encoding='utf-8') as writer:
        writer.write(f"""<head> <meta charset="utf-8 </head>
            {body_str}
        """)
        html_to_pdf(save_file_name)
        global i
        print(f'【info】: {author_name}第{i}篇博文{title}-{article_id}.html保存文件成功')
        i += 1


# 循环爬取分页html
for page_no in range(MAX_PAGE_NUM):
    try:
        data = {
            'page': '2',
            'size': '20',
            'businessType': 'blog',
            'orderby': '',
            'noMore': 'false',
            'year': '',
            'month': '',
            'username': author_name}
        pages_dict = sess.get('https://blog.csdn.net/community/home-api/v1/get-business-list', params=data).json()
        for article in pages_dict['data']['list']:
            article_id = article['articleId']
            title = article['title']
            crawler_blog_by(author_name,article_id,title)
            time.sleep(random.uniform(0.4,1.0))
    except Exception as e:
        print(e) #log日志文件系统，实际开发的话


```

## 4.觅知网

我们进行一下分析



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403481.png)



首先来看响应的



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403567.png)

通过比较我们发现

https://www.51miz.com/so-sucai/86201/p_1/

为第一页，p_2就是第二页



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403626.png)



这些是我们需要发送的参数



```python
url="https://www.51miz.com/index.php?m=relate&ajax=1"
params={
    'pagesource': 'advice',
    'keyword': '跑车',
    'keyword_id': '',#这里经过测试是可以为空的
    'plate_id': '0',
    'plate_path': 'sucai',
    'where_search': 'all'
}
```

之后是对这些进行下载

```python
headers = {
    'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.58"
}

response1 = requests.post(url=url, params=params, headers=headers)
index_dict = response1.json()
for item in index_dict:
    print(f"【开始下载】 : 类别为{item['word']}的图片")
    index_url = f"{item['url'][:-5]}/p_{i}"
    response2 = requests.get(index_url)
    html_parser = lxml.etree.HTMLParser
    html = lxml.etree.fromstring(response2.text, parser=html_parser)
    titles=html.xpath("//a[@class='image-box']//img/@title")
    html.xpath("//img[@class='lazyload']/@data-original")
    if not os.path.exists('picture'):
        os.mkdir('picture')
```

之后我们开始持久化存储

```python
for title,pic_url in zip(titles,pic_url):
    try:
        png_url=pic_url[:pic_url.index('!')] #提取！以前的
        pic_stream = requests.get(f'https:{png_url}', stream=True)
        title=title[:200] #文件名限制为255
        with open(os.path.join('picture',title+".png"),'wb+') as writer:
            writer.write(pic_stream.raw.read())
            print(f"【INFO】{title}下载成功")
            time.sleep(random.uniform(0.1,0.4))
    except Exception as e:
        with open("error.log",'a') as fo:
            fo.write(str(e))
```

至此就可以了。下面我们来看一个全部的代码



```python
import time
import random
import requests
import lxml.etree
import os

key_word = input("输入下载图片的关键词")
page_n = int(input("输入要爬取的网页数量"))
url = "https://www.51miz.com/index.php?m=relate&ajax=1"
params = {
    'pagesource': 'advice',
    'keyword': key_word,
    'keyword_id': '',  # 这里经过测试是可以为空的
    'plate_id': '0',
    'plate_path': 'sucai',
    'where_search': 'all'
}
headers = {
    'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.58"
}
response1 = requests.post(url=url, params=params, headers=headers)
index_dict = response1.json()
each_class_n = page_n // len(index_dict) + 1
for item in index_dict:
    j = 1
    while j <= each_class_n:
        print(f"【开始下载】 : 类别为{item['word']}的图片")
        index_url = f"{item['url'][:-5]}/p_{j}"
        response2 = requests.get(index_url,headers=headers)
        html_parser = lxml.etree.HTMLParser()
        html = lxml.etree.fromstring(response2.text, parser=html_parser)
        titles = html.xpath("//a[@class='image-box']//img/@title")
        pic_url = html.xpath("//img[@class='lazyload ']/@data-original")
        if not os.path.exists(key_word):
            os.mkdir(key_word)
        for title, pic_url in zip(titles, pic_url):
            try:
                png_url = pic_url[:pic_url.index('!')]  # 提取！以前的
                pic_stream = requests.get(f'https:{png_url}', stream=True)
                title = title[:200]  # 文件名限制为255
                with open(os.path.join(key_word, title + ".png"), 'wb+') as writer:
                    writer.write(pic_stream.raw.read())
                    print(f"【INFO】{title}下载成功")
                    time.sleep(random.uniform(0.1, 0.4))
            except Exception as e:
                with open("error.log", 'a') as fo:
                    fo.write(str(e))
    j += 1

```

## 5.微博超话爬取

![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403701.png)



我们发现这里是有一个高级搜素的。这个可以作为我们的一个，爬取的思路



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403797.png)

很显然是没有加密过的。



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403877.png)

之后去网页上定位一下内容。

同时对于展开的这个问题



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403918.png)



就是很简单。俩个不一样的。

### 构造小时对



```python
import calendar


def create_hour_list(year):
    data_list=[]
    for month in range(1,13):
        month_days=calendar.monthrange(year,month)[1]#返回多少天
        for day in range(1,month_days+1):
            ymd=f"{year}-{month:>02}-{day:>02}"
            data_list.append(ymd)
    day_hour_list=[f"{day}-{hour}" for day in data_list for hour in range(24)]
    return day_hour_list

if __name__ == '__main__':
    hour_list = create_hour_list(2023)
    print(hour_list)
```

这个构造很简单。这里就不做介绍了。

返回格式就是



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403969.png)



和这个一样的



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403097.png)

这个就算是构造完成了。



```python
if __name__ == '__main__':
    year=int(input('输入要查询微博的年份'))
    key_word=input('输入要爬取的主题词')
    crawler(key_word,year=year,only_chaohua=True)
```

之后我们根据这个格式来写我们的代码

### 主要代码

这里需要说一个巧妙的地方

```python
year_hour_pair=list(zip(year_hour[:-1],year_hour[1:]))
```

这段代码是将一个名为year_hour的列表进行处理，并生成一个新的列表year_hour_pair。这个新列表包含了year_hour中相邻元素的配对。

具体来说，zip(year_hour[:-1], year_hour[1:])的作用是将year_hour列表的第一个元素与第二个元素配对，第二个元素与第三个元素配对，以此类推，直到倒数第二个元素与最后一个元素配对。这样就生成了一系列的元组，每个元组中包含了相邻的两个元素。

举个例子，假设year_hour列表的内容为[2021, 2022, 2023, 2024, 2025]，那么year_hour_pair列表的内容将会是[(2021, 2022), (2022, 2023), (2023, 2024), (2024, 2025)]。

我们来看一下请求头：



```python
headers = {
    'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.58",
    'referer': f"https://s.weibo.com/weibo/?q={key_word}&typeall=1&suball=1&timescope=custom:{start_hour}:{end_hour}&Refer=g",
    'cookie': "login_sid_t=4cf9918868f99a80993ee8db70ea9978; cross_origin_proto=SSL; _s_tentry=passport.weibo.com; Apache=4295370010248.756.1688354182480; SINAGLOBAL=4295370010248.756.1688354182480; ULV=1688354182482:1:1:1:4295370010248.756.1688354182480:; SUB=_2A25Jpk5ADeRhGeFL61MT9ybOzDiIHXVq0jiIrDV8PUNbmtANLVDBkW9NQrM7Yw-SB6k32M9TnUFEl3ZR1SRytf8I; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WF97F3KxUrKVsFgiw-BcK.W5JpX5KzhUgL.FoMfeh2ES0nES0B2dJLoIp7LxKML1KBLBKnLxKqL1hnLBoMNSK5peoMReoMX; ALF=1719890320; SSOLoginState=1688354320"
}
```

用这三个就是可以的

我们这里介绍一下cookie。



```python
page_no=1
while page_no < ASSUME_HOUR_MAX_PAGE_N:
    url=f"https://s.weibo.com/weibo/?q={key_word}&typeall=1&suball=1&timescope=custom:{start_hour}:{end_hour}&Refer=g&page={page_no}:

    try:
        response = sess.get(url, headers=headers, verify=False, timeout=50)
        soup = BeautifulSoup(response.text, 'html5lib')
        error_bs = soup.find('div', attrs={'class': 'm-error'})
        if error_bs:
            print(f"{start_hour}到{end_hour}时间区域内发布的{key_word}超话数据,一共{page_no}页")
            break

        soup.findAll('div',attrs={'action-type':'feed_list_item'})

```

这里用soup

假如出错的话，有一个

error_bs这个。假如有这个的话，就说明已经爬取完了。



```python
all_bs_list = soup.findAll('div', attrs={'action-type': 'feed_list_item'})
```

这里是所有的内容

之后进行一个查找



```python
for item_bs in all_bs_list:
    if only_chaohua:
        topic_from_href=item_bs.find('a',attrs={'href':'https://huati.weibo.com/11256'})
        topic_from=item_bs.find('a',text=f"{key_word}超话")
    if topic_from_href or topic_from:
        name_bs = item_bs.find('a', attrs={'class': 'name'})
        name = item_bs.get('nick-name')
        item_bs.find('p',attrs={'node-type': 'feed_list_content_full'})
        if not name_bs:
            content_bs = item_bs.find('p', attrs={'node-type': 'feed_list_content'})
        content = content_bs.get_text().strip()
```



很简单的就不分析了。

之后持久化存储

```python
 with open(f'{year}-{key_word}-微博.csv','a') as appender:
            appender.write(f"{name},{content},{url}\n")
page_no+=1
```





之后是错误的处理



```python
except Exception as e:
    with open('error_log.log','a') as error_log:
        error_log.write(str(e)+"\n")
```

下面我们来看全部的代码

```
import calendar

import requests
from bs4 import BeautifulSoup
import urllib3


urllib3.disable_warnings()

def create_hour_list(year):
    data_list = []
    for month in range(1, 13):
        month_days = calendar.monthrange(year, month)[1]  # 返回多少天
        for day in range(1, month_days + 1):
            ymd = f"{year}-{month:>02}-{day:>02}"
            data_list.append(ymd)
    day_hour_list = [f"{day}-{hour}" for day in data_list for hour in range(24)]
    return day_hour_list


def crawler(key_word, year, only_chaohua):
    ASSUME_HOUR_MAX_PAGE_N = 100
    year_hour = create_hour_list(year)
    # 组建相邻元素来进行配对
    year_hour_pair = list(zip(year_hour[:-1], year_hour[1:]))
    sess = requests.Session()
    for start_hour, end_hour in year_hour_pair:
        key_word_code = key_word.strip().encode('utf-8')
        headers = {
            'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.58",
            'referer': f"https://s.weibo.com/weibo/?q={key_word_code}&typeall=1&suball=1&timescope=custom:{start_hour}:{end_hour}&Refer=g",
            'cookie': "login_sid_t=4cf9918868f99a80993ee8db70ea9978; cross_origin_proto=SSL; _s_tentry=passport.weibo.com; Apache=4295370010248.756.1688354182480; SINAGLOBAL=4295370010248.756.1688354182480; ULV=1688354182482:1:1:1:4295370010248.756.1688354182480:; SUB=_2A25Jpk5ADeRhGeFL61MT9ybOzDiIHXVq0jiIrDV8PUNbmtANLVDBkW9NQrM7Yw-SB6k32M9TnUFEl3ZR1SRytf8I; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WF97F3KxUrKVsFgiw-BcK.W5JpX5KzhUgL.FoMfeh2ES0nES0B2dJLoIp7LxKML1KBLBKnLxKqL1hnLBoMNSK5peoMReoMX; ALF=1719890320; SSOLoginState=1688354320"
        }
        page_no = 1
        while page_no < ASSUME_HOUR_MAX_PAGE_N:
            url = f"https://s.weibo.com/weibo/?q={key_word}&typeall=1&suball=1&timescope=custom:{start_hour}:{end_hour}&Refer=g&page={page_no}"

            try:
                response = sess.get(url, headers=headers, verify=False, timeout=50)
                soup = BeautifulSoup(response.text, 'html5lib')
                error_bs = soup.find('div', attrs={'class': 'm-error'})
                if error_bs:
                    print(f"{start_hour}到{end_hour}时间区域内发布的{key_word}超话数据,一共{page_no}页")
                    break
                all_bs_list = soup.findAll('div', attrs={'action-type': 'feed_list_item'})

                for item_bs in all_bs_list:
                    if only_chaohua:
                        topic_from_href=item_bs.find('a',attrs={'href':'https://huati.weibo.com/11256'})
                        topic_from = item_bs.find('a', string=f"{key_word}超话")

                    if topic_from_href or topic_from:
                        name_bs = item_bs.find('a', attrs={'class': 'name'})
                        name = name_bs.get('nick-name')
                        content_bs = item_bs.find('p', attrs={'node-type': 'feed_list_content_full'})
                        if not content_bs:
                            content_bs = item_bs.find('p', attrs={'node-type': 'feed_list_content'})
                        content = content_bs.get_text().strip()
                        with open(f'{year}-{key_word}-微博.csv','a',encoding='utf-8') as appender:
                            appender.write(f"{name},{content},{url}\n")
                page_no+=1
            except Exception as e:
                with open('error_log.log','a') as error_log:
                    error_log.write(str(e)+"\n")
if __name__ == '__main__':
    year = int(input('输入要查询微博的年份'))
    key_word = input('输入要爬取的主题词')
    crawler(key_word, year=year, only_chaohua=True)

```



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403491.png)

## 6.关于图怪网去水印

当时本来想去可画上做一个东西的，发现没有想要的素材，于是去搜索了一下，发现了这个网站。这个如果不开会员的话。



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403528.png)



是有一个浅浅的水印，简单的进行一个分析，首先我在想她是不是一个div的元素，但是找了半天没有找见。后来把她放到了ajax请求上。



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403581.png)

发现了这个，我们右键把她阻断了



![](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403643.png)



就没有了。

至于你问如何拿到这个图。

截屏是最快的方法。

其他的方法目前研究不出来。

## 7.第一试卷网

爬取第一试卷网网站

免费试卷 / 英语试卷 / 六年级_第一试卷网 (shijuan1.com)

![image-20230716114503545](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403703.png)

先是通过了正则表达式，来获取到了每个试卷的子链接。

```python
import re
import requests
num=1
url="https://www.shijuan1.com/a/sjyy6/list_609_"+str(num)+".html"
resp=requests.get(url)
resp.encoding=resp.apparent_encoding

obj=re.compile(r"<td width='52%' height='23'><a href=\"(?P<href>.*?)"
               r"\" class=\"title\" target='_blank'>",re.S)
result=obj.finditer(resp.text)
#定义一个爬虫
child_href_list=[]
for it in result:
   ul=it.group('href')


```

之后对子页面进行提取

```python
#提取子页面
for href in child_href_list:
    child_resp=requests.get(href)
    child_resp.encoding=child_resp.apparent_encoding
    result2=obj2.search(child_resp.text,re.S)
    downloadadress=domain+result2.group('download')
    print(downloadadress)

```

对于提取到的东西，需要进行下载。

一个下载的模板。

原理就是转换为二进制。



```python
#获取试卷
img_resp = requests.get(downloadadress)
img_picture_bite = img_resp.content  # 这里拿到的是字节
img_name = downloadadress.split("/")[-1]  # 拿到url的最后一个/以后的内容
with open(img_name, mode="wb") as f:
    f.write(img_picture_bite)  # 将字节写入到文件中
print("over!", img_name)
sleep(1)  # 为了防止服务器不把这个ip拉黑。模拟人工访问
break
```





最后是全部的源码





```python
import re
import requests
from time import sleep
num = 1
url = "https://www.shijuan1.com/a/sjyy6/list_609_" + str(num) + ".html"
domain="https://www.shijuan1.com"
resp = requests.get(url)
resp.encoding = resp.apparent_encoding

obj = re.compile(r"<td width='52%' height='23'><a href=\"(?P<href>.*?)"
                 r"\" class=\"title\" target='_blank'>", re.S)

obj2=re.compile(r'<li><a href="(?P<download>.*?)" target="_blank"')

result = obj.finditer(resp.text)

# 定义一个爬虫进行子页面的爬取
child_href_list = []
for it in result:
    ul = it.group('href')
    child_href=domain+it.group('href')
    child_href_list.append(child_href)
    print(child_href_list)
    break
#提取子页面
for href in child_href_list:
    child_resp=requests.get(href)
    child_resp.encoding=child_resp.apparent_encoding
    result2=obj2.search(child_resp.text,re.S)
    downloadadress=domain+result2.group('download')
    print(downloadadress)
    #获取试卷
    img_resp = requests.get(downloadadress)
    img_picture_bite = img_resp.content  # 这里拿到的是字节
    img_name = downloadadress.split("/")[-1]  # 拿到url的最后一个/以后的内容
    with open(img_name, mode="wb") as f:
        f.write(img_picture_bite)  # 将字节写入到文件中
    print("over!", img_name)
    sleep(1)  # 为了防止服务器不把这个ip拉黑。模拟人工访问
    break

```

这里用了break是为了测试方便，当大面积的爬取的时候，不需要break



成果也是非常的明显。

## 8.实战豆瓣图书爬取新书速递

这个用的是scrapy

![image-20230716135047326](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403894.png)

网站再这里：[豆瓣-新书速递](https://book.douban.com/latest?tag=%E5%85%A8%E9%83%A8)
之后我们需要拿这个，并且子页面的全部东西。

```python
def parse(self, response):  
    books=response.xpath('//ul[@class="chart-dashed-list"]/li')  
    print(books)  
    for book in books:  
        title=book.xpath('.//h2/a/text()').get()  
        link=book.xpath('.//h2/a/@href').get()
```

这个就是一个简单的子页面获取。

```python
def parse(self, response):  
    books=response.xpath('//ul[@class="chart-dashed-list"]/li')  
    for book in books:  
        link=book.xpath('.//h2/a/@href').get()  
        yield scrapy.Request(url=link,callback=self.pares_details)  
  
def pares_details(self,response):  
    title = response.xpath('//*[@id="wrapper"]/h1/span/text()').get()  
    publisher=response.xpath('//*[@id="info"]/a[1]/text()').get()  
    print(title,publisher)
```

这样就算是可以爬取一页了。
下面我们来进行分页数据解析
![image-20230716135113282](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403941.png)
我们从这个后一页来当做一个突破点
![image-20230716135121962](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403993.png)
输出的是这个![image-20230716135129828](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202308071403057.png)
我们发现在第四页就停住了

```python
if next_url is not None:  
    next_url=response.urljoin(next_url)  
    print(next_url)  
    yield scrapy.Request(url=next_url, callback=self.parse)  
else:  
    next_url=response.xpath('//*[@id="content"]/div/div[1]/div[4]/span[3]/a/@href').get()  
    next_url = response.urljoin(next_url)  
    print(next_url)  
    yield scrapy.Request(url=next_url, callback=self.parse)
```

我们可以发现，这个xpath有时候的是span[3] 有时候是span[4]
所以我们用ifelse
下面就是完整的代码：

```python
import scrapy  
  
  
class AppSpider(scrapy.Spider):  
    name = "app"  
    allowed_domains = ["book.douban.com"]  
    start_urls = ["https://book.douban.com/latest"]  
  
    def parse(self, response):  
        books=response.xpath('//ul[@class="chart-dashed-list"]/li')  
        for book in books:  
            link=book.xpath('.//h2/a/@href').get()  
  
            yield scrapy.Request(url=link,callback=self.pares_details)  
        next_url=response.xpath('//*[@id="content"]/div/div[1]/div[4]/span[4]/a/@href').get()  
        if next_url is not None:  
            next_url=response.urljoin(next_url)  
            print(next_url)  
            yield scrapy.Request(url=next_url, callback=self.parse)  
        else:  
            next_url=response.xpath('//*[@id="content"]/div/div[1]/div[4]/span[3]/a/@href').get()  
            next_url = response.urljoin(next_url)  
            print(next_url)  
            yield scrapy.Request(url=next_url, callback=self.parse)  
  
    def pares_details(self,response):  
        title = response.xpath('//*[@id="wrapper"]/h1/span/text()').get()  
        publisher=response.xpath('//*[@id="info"]/a[1]/text()').get()  
        print(title,publisher)
```

需要注意的是，这个需要用代理ip，或者是sleep一下再去发请求，不然会造成被服务器拉黑。

## 9.抓取m3u8视频

### 1、思路分析

视频url：https://www.9meiju.cc/mohuankehuan/shandianxiadibaji/1-1.html

1. 打开网址分析当前视频是由多个片段组成还是单独一个视频 如果是一个单独视频，则找到网址，直接下载即可，如果为多个片段的视频，则需要找到片段的文件进行处理，本案例以m3u8为例
2. 找到m3u8文件后进行下载，下载后打开文件分析是否需要秘钥，需要秘钥则根据秘钥地址进行秘钥下载，然后下载所有ts文件
3. 合并所有视频

### 2、实现

#### 分析index.m3u8

+ 通过网络查找发现有俩个m3u8文件

  url分别为

  https://new.qqaku.com/20211117/iHVkqQMI/index.m3u8

  https://new.qqaku.com/20211117/iHVkqQMI/2523kb/hls/index.m3u8

  通过分析 第一个index.m3u8请求返回的内容中包含了第二个m3u8请求的url地址 

  也就是说通过第一个index.m3u8url请求返回包含第二个index.m3u8文件地址，通过拼接请求第二个index.m3u8后 返回了包含当前所有ts文件的地址内容

  现在分析出了第二个真正的index.m3u8的地址，但是第一个地址从哪里来的呢，别慌，接下来我们来查找一下第一个url是从哪里来的

  ![image-20220708105559202](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202309171243789.png)

  ![image-20220708105618510](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202309171243460.png)

+ 查找第一个index.m3u8的url地址

  打开source

  发现url存在页面源代码中的js里  知道了位置，在代码中通过正则匹配就可以获取到了 

  现在我们缕一下思路，通过页面源代码可以找到第一个index.m3u8的url，通过请求返回包含第二个index.m3u8文件的url内容，进行拼接，请求第二个m3u8的url，以此返回所有的ts内容

  ![image-20220708110048589](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202309171243077.png)



### 3、代码实现

#### 3.1 获取最后一个m3u8的url地址

```python
import re
from urllib.parse import urljoin

import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

session = requests.Session()
session.get('https://www.9meiju.cc/', headers=headers)

url = 'https://www.9meiju.cc/mohuankehuan/shandianxiadibaji/1-2.html'
response = session.get(url, headers=headers)
response.encoding = 'UTF-8'
data = response.text
# print(data)
'''
<script>
var zanpiancms_player = {"player":"\/public\/","url":"https:\/\/new.qqaku.com\/20211124\/nLwncbZW\/index.m3u8","next":"https:\/\/www.9meiju.cc\/mohuankehuan\/shandianxiadibaji\/1-3.html","name":"wjm3u8","apiurl":null,"adtime":"0","adurl":"","copyright":0,"danmu":{"status":0}};
</script>
'''
# 正则抓取上面的源代码中的m3u8的url
m3u8_uri = re.search('"url":"(.+?index.m3u8)"', data).group(1).replace('\\', '')

# 写入文件 分析当前的页面源代码
with open('99.html', 'w', encoding='UTF-8') as f:
    # 写入response.content bytes二进制类型
    f.write(response.content.decode('UTF-8'))

# 请求可以获取index.m3u8文件
response = session.get(m3u8_uri, headers=headers)
with open('m3u8_uri.text', 'w', encoding='UTF-8') as f:
    # 写入response.content bytes二进制类型
    f.write(response.content.decode('UTF-8'))
response.encoding = 'UTF-8'
data = response.text

# 拆分返回的内容获取真整的index.m3u8文件的url
url = data.split('/', 3)[-1]
print(data)
print('m3u8_uri', m3u8_uri)
print('url', url)
print(urljoin(m3u8_uri, url))
```

#### 3.2  多线程下载ts文件与视频合并

```python
import time
import requests
import os
from concurrent.futures import ThreadPoolExecutor, wait

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36"
}


def down_video(url, i):
    '''
    下载ts文件
    :param url:
    :param i:
    :return:
    '''
    # print(url)
    # 下载ts文件
    resp = requests.get(url, headers=headers)
    with open(os.path.join(path, str(i)+'.ts'), mode="wb") as f3:
        f3.write(resp.content)
    print('{} 下载完成！'.format(url))


def download_all_videos(url, path):
    '''
    下载m3u8文件以及多线程下载ts文件
    :param url:
    :param path:
    :return:
    '''
    # 请求m3u8文件进行下载
    resp = requests.get(url, headers=headers)
    with open("first.m3u8", mode="w", encoding="utf-8") as f:
        f.write(resp.text)
    if not os.path.exists(path):
        os.mkdir(path)
    # 开启线程 准备下载
    pool = ThreadPoolExecutor(max_workers=50)
    # 1. 读取文件
    tasks = []
    i = 0
    with open("first.m3u8", mode="r", encoding="utf-8") as f:
        for line in f:
            # 如果不是url 则走下次循环
            if line.startswith("#"):
                continue
            print(line, i)
            # 开启线程
            tasks.append(pool.submit(down_video, line.strip(), i))
            i += 1
    print(i)
    # 统一等待
    wait(tasks)


# 处理m3u8文件中的url问题
def do_m3u8_url(path, m3u8_filename="index.m3u8"):
    # 这里还没处理key的问题
    if not os.path.exists(path):
        os.mkdir(path)
    # else:
        # shutil.rmtree(path)
        # os.mkdir(path)
    with open(m3u8_filename, mode="r", encoding="utf-8") as f:
        data = f.readlines()

    fw = open(os.path.join(path, m3u8_filename), 'w', encoding="utf-8")
    abs_path = os.getcwd()
    i = 0
    for line in data:
        # 如果不是url 则走下次循环
        if line.startswith("#"):
            # 判断处理是存在需要秘钥
            fw.write(line)
        else:
            fw.write(f'{abs_path}/{path}/{i}.ts\n')
            i += 1
    

def merge(filePath, filename='output'):
    '''
    进行ts文件合并 解决视频音频不同步的问题 建议使用这种
    :param filePath:
    :return:
    '''
    os.chdir(path)
    cmd = f'ffmpeg -i index.m3u8 -c copy {filename}.mp4'
    os.system(cmd)



if __name__ == '__main__':
    # 抓取99美剧闪电侠
    # ts文件存储目录
    path = 'ts'
    url = 'https://new.qqaku.com/20211124/nLwncbZW/1100kb/hls/index.m3u8'
    # 下载m3u8文件以及ts文件
    download_all_videos(url, path)
    do_m3u8_url(path)
    # 文件合并
    merge(path, 'ts2')
    print('over')
```

注意：当前视频合并所用的工具为ffmpeg  如需安装 查看我的另外一篇博客[ffmpeg的使用](https://www.cnblogs.com/xialigang/p/16450222.html) 

#### 3.3 合并获取上面俩个代码段的代码

```python
import re
from urllib.parse import urljoin
import requests
import os  # 执行cmd/控制台上的命令
from concurrent.futures import ThreadPoolExecutor, wait
from retrying import retry


def get_m3u8_url(url):
    '''
    获取页面中m3u8的url
    :param url: 电影页面的url
    :return:
    '''
    session = requests.Session()
    # 访问首页获取cookie
    session.get('https://www.9meiju.cc/', headers=headers)
    # url = 'https://www.9meiju.cc/mohuankehuan/shandianxiadibaji/1-2.html'
    response = session.get(url, headers=headers)
    response.encoding = 'UTF-8'
    data = response.text
    # print(data)
    m3u8_uri = re.search('"url":"(.+?index.m3u8)"', data).group(1).replace('\\', '')

    # 写入文件 分析当前的页面源代码
    # with open('99.html', 'w', encoding='UTF-8') as f:
        # 写入response.content bytes二进制类型
        # f.write(response.content.decode('UTF-8'))

    # 请求可以获取index.m3u8文件
    response = session.get(m3u8_uri, headers=headers)
    # with open('m3u8_uri.text', 'w', encoding='UTF-8') as f:
        # 写入response.content bytes二进制类型
        # f.write(response.content.decode('UTF-8'))
    response.encoding = 'UTF-8'
    data = response.text
    # 拆分返回的内容获取真整的index.m3u8文件的url
    # 注意 一定要strip
    url = data.split('/', 3)[-1].strip()
    print(data)
    print('m3u8_uri', m3u8_uri)
    url = urljoin(m3u8_uri, url)
    print('url', url)
    return url

@retry(stop_max_attempt_number=3)
def down_video(url, i):
    '''
    下载ts文件
    :param url:
    :param i:
    :return:
    '''
    # print(url)
    # 下载ts文件
    # try:
    resp = requests.get(url, headers=headers)
    with open(os.path.join(path, str(i)+'.ts'), mode="wb") as f3:
        f3.write(resp.content)
    assert resp.status_code == 200


def download_all_videos(url, path):
    '''
    下载m3u8文件以及多线程下载ts文件
    :param url:
    :param path:
    :return:
    '''
    # 请求m3u8文件进行下载
    resp = requests.get(url, headers=headers)
    with open("index.m3u8", mode="w", encoding="utf-8") as f:
        f.write(resp.content.decode('UTF-8'))
    if not os.path.exists(path):
        os.mkdir(path)
    # 开启线程 准备下载
    pool = ThreadPoolExecutor(max_workers=50)
    # 1. 读取文件
    tasks = []
    i = 0
    with open("index.m3u8", mode="r", encoding="utf-8") as f:
        for line in f:
            # 如果不是url 则走下次循环
            if line.startswith("#"):
                continue
            print(line, i)
            # 开启线程
            tasks.append(pool.submit(down_video, line.strip(), i))
            i += 1
    print(i)
    # 统一等待
    wait(tasks)
    # 如果阻塞可以给一个超时参数
    # wait(tasks, timeout=1800)


def do_m3u8_url(path, m3u8_filename="index.m3u8"):
    # 这里还没处理key的问题
    if not os.path.exists(path):
        os.mkdir(path)
    # else:
    # shutil.rmtree(path)
    # os.mkdir(path)
    with open(m3u8_filename, mode="r", encoding="utf-8") as f:
        data = f.readlines()

    fw = open(os.path.join(path, m3u8_filename), 'w', encoding="utf-8")
    abs_path = os.getcwd()
    i = 0
    for line in data:
        # 如果不是url 则走下次循环
        if line.startswith("#"):
            fw.write(line)
        else:
            fw.write(f'{abs_path}/{path}/{i}.ts\n')
            i += 1

def merge(path, filename='output'):
    '''
    进行ts文件合并 解决视频音频不同步的问题 建议使用这种
    :param filePath:
    :return:
    '''
    os.chdir(path)
    cmd = f'ffmpeg -i index.m3u8 -c copy {filename}.mp4'
    os.system(cmd)

if __name__ == '__main__':
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

    # 电影的url 返回index.m3u8的url地址
    url = get_m3u8_url('https://www.9meiju.cc/mohuankehuan/shandianxiadibaji/1-2.html')

    # ts文件存储目录
    path = 'ts'
    # 下载m3u8文件以及ts文件
    download_all_videos(url, path)
    do_m3u8_url(path)
    # 文件合并
    merge(path, '第二集')
    print('over')
```



### 4、注意事项

4.1 说明

在获取index.m3u8文件的内容时，有的文件内容会显示...jpg/png的情况，并没显示...ts，那么遇到这种情况需要单独处理 内容如下：

![image-20220708111809978](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202309171243993.png)

这种情况使用上面的代码就无法进行正常合并，合并后的视频无法播放

但使用ffprobe分析，发现识别为png，进而导致无法正常拼接

![image-20220708112001167](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202309171243702.png)

**在这种情况下，只需要将其中PNG文件头部分全部使用FF填充，即可处理该问题**

填充后的效果如图

![image-20220708112029091](https://xiaou-1305448902.cos.ap-nanjing.myqcloud.com/img/202309171243584.png)

#### 4.2 使用代码进行处理

```python
# 解析伪装成png的ts
def resolve_ts(src_path, dst_path):
    '''
    如果m3u8返回的ts文件地址为
    https://p1.eckwai.com/ufile/adsocial/7ead0935-dd4f-4d2f-b17d-dd9902f8cc77.png
    则需要下面处理后 才能进行合并
    原因在于 使用Hexeditor打开后，发现文件头被描述为了PNG
    在这种情况下，只需要将其中PNG文件头部分全部使用FF填充，即可处理该问题
    :return:
    '''
    if not os.path.exists(dst_path):
        os.mkdir(dst_path)
    file_list = sorted(os.listdir(src_path), key=lambda x: int(x.split('.')[0]))
    for i in file_list:
        origin_ts = os.path.join(src_path, i)
        resolved_ts = os.path.join(dst_path, i)
        try:
            infile = open(origin_ts, "rb")  # 打开文件
            outfile = open(resolved_ts, "wb")  # 内容输出
            data = infile.read()
            outfile.write(data)
            outfile.seek(0x00)
            outfile.write(b'\xff\xff\xff\xff')
            outfile.flush()
            infile.close()  # 文件关闭
            outfile.close()
        except:
            pass
        print('resolve ' + origin_ts + ' success')
```

#### 4.3 完整代码

```python
import shutil
import time
from urllib.parse import urljoin

import requests
import os
import re
from concurrent.futures import ThreadPoolExecutor, wait


def get_m3u8_url(url):
    '''
    获取页面中m3u8的url
    :param url: 电影页面的url
    :return:
    '''
    session = requests.Session()
    # 访问首页获取cookie
    session.get('https://www.9meiju.cc/', headers=headers)
    # url = 'https://www.9meiju.cc/mohuankehuan/shandianxiadibaji/1-2.html'
    response = session.get(url, headers=headers)
    response.encoding = 'UTF-8'
    data = response.text
    # print(data)
    m3u8_uri = re.search('"url":"(.+?index.m3u8)"', data).group(1).replace('\\', '')


    # 请求可以获取index.m3u8文件
    response = session.get(m3u8_uri, headers=headers)
    response.encoding = 'UTF-8'
    data = response.text
    # 拆分返回的内容获取真整的index.m3u8文件的url
    # 注意 一定要strip
    url = data.split('/', 3)[-1].strip()
    print(data)
    print('m3u8_uri', m3u8_uri)
    url = urljoin(m3u8_uri, url)
    print('url', url)
    return url

def down_video(url, i):
    '''
    下载ts文件
    :param url:
    :param i:
    :return:
    '''
    # print(url)
    # 下载ts文件
    resp = requests.get(url, headers=headers)
    with open(os.path.join(path, str(i)+'.ts'), mode="wb") as f3:
        f3.write(resp.content)
    # print('{} 下载完成！'.format(url))


def download_all_videos(url, path):
    '''
    下载m3u8文件以及多线程下载ts文件
    :param url:
    :param path:
    :return:
    '''
    # 请求m3u8文件进行下载
    resp = requests.get(url, headers=headers)
    with open("index.m3u8", mode="w", encoding="utf-8") as f:
        f.write(resp.content.decode('UTF-8'))
    if not os.path.exists(path):
        os.mkdir(path)
    # 开启线程 准备下载
    pool = ThreadPoolExecutor(max_workers=50)
    # 1. 读取文件
    tasks = []
    i = 0
    with open("index.m3u8", mode="r", encoding="utf-8") as f:
        for line in f:
            # 如果不是url 则走下次循环
            if line.startswith("#"):
                continue
            print(line, i)
            # 开启线程
            tasks.append(pool.submit(down_video, line.strip(), i))
            i += 1
    print(i)
    # 统一等待
    wait(tasks)



# 解析伪装成png的ts
def resolve_ts(src_path, dst_path):
    '''
    如果m3u8返回的ts文件地址为
    https://p1.eckwai.com/ufile/adsocial/7ead0935-dd4f-4d2f-b17d-dd9902f8cc77.png
    则需要下面处理后 才能进行合并
    原因在于 使用Hexeditor打开后，发现文件头被描述为了PNG
    在这种情况下，只需要将其中PNG文件头部分全部使用FF填充，即可处理该问题
    :return:
    '''
    if not os.path.exists(dst_path):
        os.mkdir(dst_path)
    file_list = sorted(os.listdir(src_path), key=lambda x: int(x.split('.')[0]))
    for i in file_list:
        origin_ts = os.path.join(src_path, i)
        resolved_ts = os.path.join(dst_path, i)
        try:
            infile = open(origin_ts, "rb")  # 打开文件
            outfile = open(resolved_ts, "wb")  # 内容输出
            data = infile.read()
            outfile.write(data)
            outfile.seek(0x00)
            outfile.write(b'\xff\xff\xff\xff')
            outfile.flush()
            infile.close()  # 文件关闭
            outfile.close()
        except:
            pass
        """
        else:
            # 删除目录
            shutil.rmtree(src_path)
            # 将副本重命名为正式文件
            os.rename(dst_path, dst_path.rstrip('2'))
        """
        print('resolve ' + origin_ts + ' success')


# 处理m3u8文件中的url问题
def do_m3u8_url(path, m3u8_filename="index.m3u8"):
    # 这里还没处理key的问题
    if not os.path.exists(path):
        os.mkdir(path)

    with open(m3u8_filename, mode="r", encoding="utf-8") as f:
        data = f.readlines()

    fw = open(os.path.join(path, m3u8_filename), 'w', encoding="utf-8")
    abs_path = os.getcwd()
    i = 0
    for line in data:
        # 如果不是url 则走下次循环
        if line.startswith("#"):
            fw.write(line)
        else:
            fw.write(f'{abs_path}/{path}/{i}.ts\n')
            i += 1

def merge(path, filename='output'):
    '''
    进行ts文件合并 解决视频音频不同步的问题 建议使用这种
    :param filePath:
    :return:
    '''
    os.chdir(path)
    cmd = f'ffmpeg -i index.m3u8 -c copy {filename}.mp4'
    os.system(cmd)


if __name__ == '__main__':
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36"
    }
    url = get_m3u8_url('https://www.9meiju.cc/mohuankehuan/shandianxiadibaji/1-20.html')
    # 抓取99美剧闪电侠
    # ts文件存储目录
    path = 'ts'
    # 下载m3u8文件以及ts文件
    download_all_videos(url, path)
    # 合并png的ts文件
    src_path = path
    dst_path = path+'2'
    resolve_ts(src_path, dst_path)
    do_m3u8_url(dst_path)
    merge(dst_path, '闪电侠')
    print('over')
```

### 5、解密处理

- 上面我们讲的是没有经过加密的 ts 文件，这些文件下载后直接可以播放，但经过AES-128加密后的文件下载后会无法播放，所以还需要进行解密。

- 如何判断是否需要加密？观察视频网站是否有m3u8的文件传输，下载下来并打开：

  无需解密index.m3u8文件

  ```python
  #EXTM3U
  #EXT-X-VERSION:3
  #EXT-X-TARGETDURATION:4
  #EXT-X-PLAYLIST-TYPE:VOD
  #EXT-X-MEDIA-SEQUENCE:0
  #EXTINF:3.086,
  https://hey05.cjkypo.com/20211215/FMbNtNzz/1100kb/hls/7qs6gJc0.ts
  #EXTINF:2.085,
  https://hey05.cjkypo.com/20211215/FMbNtNzz/1100kb/hls/rYpHhq0I.ts
  #EXTINF:2.085,
  https://hey05.cjkypo.com/20211215/FMbNtNzz/1100kb/hls/bfays5sw.ts
  ```

  需要解密index.m3u8文件

  index.m3u8：https://s7.fsvod1.com/20220622/5LnZiDXn/index.m3u8

  ```python
  #EXT-X-VERSION:3
  #EXT-X-TARGETDURATION:1
  #EXT-X-PLAYLIST-TYPE:VOD
  #EXT-X-MEDIA-SEQUENCE:0
  #EXT-X-KEY:METHOD=AES-128,URI="/20220418/671fJxOB/2000kb/hls/key.key" # 当前路径为解密秘钥的位置  需要使用代码拼凑成完整路径 进行请求 域名+/20220418/671fJxOB/2000kb/hls/key.key
  #EXTINF:1.235,
  /20220418/671fJxOB/2000kb/hls/kj6uqHoP.ts  # 并且这里ts的url也要拼凑完整
  #EXTINF:1.001,
  /20220418/671fJxOB/2000kb/hls/ZXX8LYPa.ts
  #EXTINF:1.001,
  /20220418/671fJxOB/2000kb/hls/sOezpD2H.ts
  #EXTINF:1.001,
  ...
  ```

- 如果你的文件是加密的，那么你还需要一个key文件，Key文件下载的方法和m3u8文件类似，如下所示 key.key 就是我们需要下载的 key 文件，并注意这里 m3u8 有2个，需要使用的是像上面一样存在 ts 文件超链接的 m3u8 文件

- **下载所有 ts 文件**，将下载好的所有的 ts 文件、m3u8、key.key 放到一个文件夹中，将 m3u8 文件改名为 index.m3u8，将 key.key 改名为 key.m3u8 。更改 index.m3u8 里的 URL，变为你本地路径的 key 文件，将所有 ts 也改为你本地的路径

  文件路径

  > project/
  >
  > ​		ts/
  >
  > ​			0.ts
  >
  > ​			1.ts
  >
  > ​			...
  >
  > ​			index.m3u8
  >
  > ​			key.m3u8

  修改后的index.m3u8内容如下所示：

  ```python
  #EXTM3U
  #EXT-X-VERSION:3
  #EXT-X-TARGETDURATION:1
  #EXT-X-PLAYLIST-TYPE:VOD
  #EXT-X-MEDIA-SEQUENCE:0
  #EXT-X-KEY:METHOD=AES-128,URI="/Users/xialigang/PycharmProjects/爬虫/抓取带秘钥的电影/ts/key.m3u8"
  #EXTINF:1.235,
  /Users/xialigang/PycharmProjects/爬虫/抓取带秘钥的电影/ts/0.ts
  #EXTINF:1.001,
  /Users/xialigang/PycharmProjects/爬虫/抓取带秘钥的电影/ts/1.ts
  #EXTINF:1.001,
  /Users/xialigang/PycharmProjects/爬虫/抓取带秘钥的电影/ts/2.ts
  ```

  处理index.m3u8内容的代码如下所示

  ```python
  import time
  from urllib.parse import urljoin
  
  import requests
  import os
  from concurrent.futures import ThreadPoolExecutor, wait
  import re
  
  headers = {
      "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36"
  }
  def down_video(url, i):
      '''
    下载ts文件
    :param url:
    :param i:
    :return:
    '''
    # print(url)
    # 下载ts文件
    resp = requests.get(url, headers=headers)
    with open(os.path.join(path, str(i) + '.ts'), mode="wb") as f3:
        f3.write(resp.content)
  # print('{} 下载完成！'.format(url))
  def download_all_videos(path, host):
    '''
    下载m3u8文件以及多线程下载ts文件
    :param url:
    :param path:
    :return:
    '''
    if not os.path.exists(path):
        os.mkdir(path)
    # 开启线程 准备下载
    pool = ThreadPoolExecutor(max_workers=50)
    # 1. 读取文件
    tasks = []
    i = 0
    with open("index.m3u8", mode="r", encoding="utf-8") as f:
        for line in f:
            # 如果不是url 则走下次循环
            if line.startswith("#"):
                continue
            line = host + line
            print(line, i)
            # 开启线程
            tasks.append(pool.submit(down_video, line.strip(), i))
            i += 1
    # 统一等待
    wait(tasks)
  # 处理m3u8文件中的url问题
  def do_m3u8url(url, path, m3u8filename="index.m3u8"):
    # 这里还没处理key的问题
    if not os.path.exists(path):
        os.mkdir(path)
  
    with open(m3u8_filename, mode="r", encoding="utf-8") as f:
        data = f.readlines()
  
    fw = open(os.path.join(path, m3u8_filename), 'w')
    abs_path = os.getcwd()
    i = 0
    for line in data:
        # 如果不是url 则走下次循环
        if line.startswith("#"):
            # 判断处理是存在需要秘钥
            if line.find('URI') != -1:
                line = re.sub('(#EXT-X-KEY:METHOD=AES-128,URI=")(.*?)"', f'\\1{os.path.join(abs_path, path)}/key.m3u8"',
                              line)
                host = url.rsplit('/', 1)[0]
                # 爬取key
                download_m3u8(host + '/key.key', os.path.join(path, 'key.m3u8'))
            fw.write(line)
        else:
            fw.write(f'{abs_path}/{path}/{i}.ts\n')
            i += 1
  def download_m3u8(url, m3u8_filename="index.m3u8", state=0):
     print('正在下载index.m3u8文件')
    resp = requests.get(url, headers=headers)
    with open(m3u8_filename, mode="w", encoding="utf-8") as f:
        f.write(resp.text)
  def merge(filePath, filename='output'):
      '''
    进行ts文件合并 解决视频音频不同步的问题 建议使用这种
    :param filePath:
    :return:
    '''
    os.chdir(path)
    cmd = f'ffmpeg -i index.m3u8 -c copy {filename}.mp4'
    os.system(cmd)
  def get_m3u8data(first_m3u8url):
    session = requests.Session()
    # 请求第一次m3u8de url
    resp = session.get(first_m3u8_url, headers=headers)
    resp.encoding = 'UTF-8'
    data = resp.text
  
    # 第二次请求m3u8文件地址 返回最终包含所有ts文件的m3u8
    second_m3u8_url = urljoin(first_m3u8_url, data.split('/', 3)[-1].strip())
    resp = session.get(second_m3u8_url, headers=headers)
    with open('index.m3u8', 'wb') as f:
        f.write(resp.content)
    return second_m3u8_url
  if __name__ == '__main__':
    # ts文件存储目录
    path = 'ts'
    # 带加密的ts文件的 index.m3u8  url
    url = 'https://s7.fsvod1.com/20220622/5LnZiDXn/index.m3u8'
    meu8_url = get_m3u8_data(url)
    # 下载m3u8文件以及ts文件
    host = 'https://s7.fsvod1.com'   # 主机地址  用于拼凑完整的ts路径和秘钥路径
    download_all_videos(path, host)
    do_m3u8_url(meu8_url, path)
  
    # 文件合并
    merge(path, '奇异博士')
    print('over')
  ```

- 这样就大功告成了！我们成功解密并使用 ffmpeg 合并了这些 ts 视频片段，实际应用场景可能和这不一样，具体网站具体分析

